import { tokenize} from "./Parser";

describe("Check the tokenizer", () => {
    test("Tokenizer", () => {

        expect(tokenize("hello").map(t => t.toWordplay()).join("\n")).toBe("hello");
        expect(tokenize("hello hello").map(t => t.toWordplay()).join("\n")).toBe("hello\n \nhello");
        expect(tokenize("hello\nhello").map(t => t.toWordplay()).join("\n")).toBe("hello\n\n\nhello");
        expect(tokenize("hello  \thello").map(t => t.toWordplay()).join("\n")).toBe("hello\n  \t\nhello");
        expect(tokenize("1").map(t => t.toWordplay()).join("\n")).toBe("1");
        expect(tokenize("-1").map(t => t.toWordplay()).join("\n")).toBe("-1");
        expect(tokenize("1.0").map(t => t.toWordplay()).join("\n")).toBe("1.0");
        expect(tokenize("-1.0").map(t => t.toWordplay()).join("\n")).toBe("-1.0");
        expect(tokenize("1,0").map(t => t.toWordplay()).join("\n")).toBe("1,0");
        expect(tokenize("-1,0").map(t => t.toWordplay()).join("\n")).toBe("-1,0");
        expect(tokenize("()[]{}:.Æ’â†‘â†“`!â€¢â€¦").map(t => t.toWordplay()).join("\n"))
            .toBe("(\n)\n[\n]\n{\n}\n:\n.\nÆ’\nâ†‘\nâ†“\n`\n!\nâ€¢\nâ€¦");
        expect(tokenize("ğŸ™ƒğŸ™‚+-Ã—Ã·%<>boomyâ‰¤â‰¥&|~").map(t => t.toWordplay()).join("\n"))
            .toBe("ğŸ™ƒ\nğŸ™‚\n+\n-\nÃ—\nÃ·\n%\n<\n>\nboomy\nâ‰¤\nâ‰¥\n&\n|\n~");
        expect(tokenize("\n   \t").map(t => t.toWordplay()).join("\n"))
            .toBe("\n\n   \t");
        expect(tokenize("/eng'hi'\"hi\"â€˜hiâ€˜Â«hiÂ»â€¹hiâ€ºâ€hiâ€œã€Œhiã€").map(t => t.toWordplay()).join("\n"))
            .toBe("/eng\n'hi'\n\"hi\"\nâ€˜hiâ€˜\nÂ«hiÂ»\nâ€¹hiâ€º\nâ€hiâ€œ\nã€Œhiã€");

    })

})